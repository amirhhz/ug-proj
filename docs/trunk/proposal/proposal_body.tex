% to be included in proposal.tex

\author{Amir H. Hajizamani}
\title{CST Part II Individual Project Proposal - Recommender Systems for Social Networks}

\newcommand{\al}{$<$}
\newcommand{\ar}{$>$}

\parindent 0pt
\parskip 6pt

\thispagestyle{empty}

\rightline{\large\emph{Amir H. Hajizamani}}
\medskip
\rightline{\large\emph{St. John's College}}
\medskip
\rightline{\large\emph{ahh29}}

\vfill

\centerline{\large Computer Science Tripos Part II Individual Project Proposal}
\vspace{0.4in}
\centerline{\Large\bf Recommender Systems for Social Networks}
\vspace{0.3in}
\centerline{\large{11 October 2010}}

\vfill

{\bf Project Originators:} Amir H. Hajizamani

\vspace{0.1in}

{\bf Resources Required:} See attached Project Resource Form

\vspace{0.5in}

{\bf Project Supervisor:} Cecilia Mascolo

\vspace{0.2in}

{\bf Signature:}

\vspace{0.5in}

{\bf Director of Studies:} Robert Mullins

\vspace{0.2in}

{\bf Signature:}

\vspace{0.5in}

{\bf Overseers:} Ann Copestake and Robert Harle

\vspace{0.2in}

{\bf Signatures:}

\vfill
\eject


\section{Introduction and Description of the Work}

In this project I will explore and implement techniques for making recommendations to members of a social community, focusing on online social networks. The motivation behind this idea is that data about an individual person or some digital content (text, audio-visual, ratings, etc.) is only as valuable as its relation to other such entities. This is true for the user, whose experience is enriched, and the network-provider, whose cultural and financial success will depend on behaviour-based analysis and interactions of its users.

% \subsection*{Dataset}
As a particular case, I will work on data gathered from Mixcloud.com whom I worked for over the summer of 2009. They are a steadily-growing and successful content distribution website, aiming to be the ``YouTube of Radio''\footnote{http://ww.mixcloud.com/about}. They provide their users with community features such as Twitter-style following, Facebook-style activity updates, favouriting of ``\emph{Cloudcasts}'' (uploaded audio content) and commenting, and a record of listening history. The user-uploaded Cloudcasts are also heavily annotated with tags, categories, text descriptions, metadata about recent listeners and more. At the time of writing, Mixcloud implement some simple mechanisms for \emph{item recommendations} and \emph{personalised recommendations} - recommending Cloudcasts similar to the one being viewed, and recommending ones matching the user's listening history. My recommender system would primarily aim to provide the functionality of \emph{social recommendations} - other users whom the current user may be interested in following or listening to based on their shared social connections, activity similarities and other metrics. The problem of recommending content to users based on the listening pattern of their social connections can also be called social recommendation and would likely use similar underlying data and metrics. It is likely that working solutions to these would draw on concept from all three types of recommendation mentioned so far to account for inherent peculiarities in the dataset. For example, two users may have common taste in music but not be well-connected in the social graph or vice-versa.

\subsection*{Wider Context}

The Web is rife with successful recommender systems with companies such as \textit{Amazon, Netflix\footnote{In 2006 Netflix challenged programmers to improve their movie recommendation engine by 10\%.}} and \textit{Pandora} (radio service backed by \textit{The Music Genome Project}\footnote{http://www.pandora.com -- The MGP uses the``genealogy'' of music to link songs and artists}) investing a lot of time and money in improving their algorithms and data quality to better recommend items of interest to their users. On the other hand, more directly community-driven websites such as \textit{del.icio.us} and \textit{flickr} have harnessed the collective intelligence of their users and their tagging of their content with keywords. Collaborative tagging leads to an organic categorisation and annotation of data which has been termed \textit{folksonomy}\footnote{http://www.vanderwal.net/folksonomy.html -- a portmanteau of \textit{folk} and \textit{taxonomy} }. In a sufficiently large and active community, the folksonomy that can be extracted becomes a good source of input for the algorithms in a recommender system. 

With the accelerating growth of social networks such as Twitter and Facebook, the value of social recommendations which aim to increase the connections between individuals is more important that it used to be: the items of interests are now other users not products on sale (though from an advertising revenue perspective the difference is blurred). Recommendation engines are nearing maturity and will soon be, if they are not already, commodity technologies as the ``social layer'' of virtual communities is completed\footnote{The ``game layer'' is the next stage in building virtual communities, the founder of SCVNGR argues - http://www.ted.com/talks/seth\_priebatsch\_the\_game\_layer\_on\_top\_of\_the\_world.html}. This means that an analysis of such systems is essential to the understanding of social networking and the Web as it experiences a paradigm shift from a \emph{search} paltform (e.g. Google) to a \emph{discovery} platform.

\section{Extension Ideas}

\subsection*{Different Datasets}

I have opted to use the Mixcloud.com dataset as it has not been analysed before, but there are many datasets publicly available which are traditionally used to experiment with recommender systems. These include the MovieLens dataset (movie ratings and tags by users), Jester Joke dataset (joke ratings), and Book-Crossing dataset (book ratings). There are potentially other datasets which I may be able to use, for instance data from \emph{MyPersonailty.info}, a collaborator with the Social Psychology department who are gathering personality data using a Facebook application, and therefore has a social context within the Facebook Graph. It is possible that I have time to analyse these datasets and implement recommender systems with a different focus - item recommendations where social links are not a part of the data (the traditional datasets), and deeper social analysis in the case of MyPersonailty data;

\subsection*{Scalability and Latency}
Thinking ahead about possible extensions for this project, a limitation\footnote{http://www.readwriteweb.com/archives/recommendation\_systems\_where\_we\_need\_to\_go.php} of some current recommender systems is the latency of the recommendations after changes in user behaviour, item data and the relationships therein. Especially for large datasets a more ``real time'' performance would be desirable for a recommender system. In particular, I know that Mixcloud's personalised recommendations for users are computed as a batch job overnight on their server, which is effective for them but I wonder if there is more elegant solution with scalable properties and taking advantage of distributed processing power. A possible solution might be to periodically and incrementally update a stored set of recommendations after a sufficient number of changes occur.

TODO: Temporal Analysis, \ldots

\section{Starting Point}

I am familiar with the RESTful\footnote{Representational State Transfer-style access to data over HTTP/1.1. This particular API responds to requests in the JSON data interchange format.} API provided by Mixcloud which I have already begun obtaining data from, and I have an understanding of how the website's codebase is put together. What I have learned from some of the Computer Science Tripos courses such as Artificial Intelligence I, Digital Communications I and Databases will likely help me along the way.

The type of algorithms I will be using in this project will be new to me and my only exposure to them is in reading the relevant literature.
  
\section{Substance and Structure of the Project}

The data for the project will be obtained over the Mixcloud API as JSON objects and stored in a documented-oriented database such as MongoDB. This will involve writing a crawler application to explore the social and content graph of Mixcloud via its API and save the relevant result in a useful way.

As I mentioned in the above section, I have no prior experience of the types of processing and algorithms used in building recommender systems and therefore I will have to spend some time examining the existing literature and open source code.

Building a recommender system involves the following broad stages: 
\begin{itemize}
 \item Identifying appropriate distance metrics to measure the similarity or relevance of item-item, item-user and user-user pairs;
 \item Computing the above metrics for the available data;
 \item Make recommendations with an appropriate filtering alogirthm, using these metrics.
\end{itemize}

\subsection*{Distance Metrics}

The techniques used to measure distance between two entities of the same type can be similar whether the entities are Cloudcasts (items) or users. These can then be used to calculate the mixed-type distances, that is, the relevance of a Cloudcast to the active user.

A distance metric for measuring item-item similarity could use the following objects as it's input data: 
\begin{itemize}
 \item The sets of users who have listened to/favourited/commented on/uploaded the two items in question
 \item The sets of tags and other metadata carried by the two items 
\end{itemize}


Similarly, a metric for measuring social relevance -- whether a user-user pairing should exist between two individuals -- can use the following input data:
\begin{itemize}
 \item Association: the two users' sets of current social links - followers, followees, profile commentors. 
 \item Taste and Activity: the two users' sets of listening history/favourites/Cloudcast comments.
 \item In-Depth Taste: the metadata carried by the users' uploads/favourites/listening history.
\end{itemize}

The actual distance metrics that I will use will be subject to implementation details. TODO: Jaccard distance, Hamming distance, Formal Concept Analysis

\subsection*{Processing the Data}

TODO: implementation and performance issues

\subsection*{Classification, Filtering}

TODO: 
Supervised Learning (training set provided): Binary classification - Bayesian networks, decision trees, Support Vector Machines, Neural networks
Unsupervised Learning (no training set): Hierarchical clustering \& choice of distance metrics
Collaborative Filtering: Memory-based (neighbourhood search, Locality Sensitive Hashing, correlation measures) vs. Model-based (see Supervised Learning and clustering)

% An initial idea as to how I might utilise my folksonomy data is by constructing bi- or tripartite graphs that link tags, content and users. The links between the items in these graphs can give us approximations of a user's preferences, or tell us which pieces of content may be related to each other, and a score can be computed about the strength of the connection between two items or users. It may be that several bipartite graphs (tags-content, content-users, users-users etc.) give better results than one very large tripartite graph and I would be interested to see how different approaches compare. The scores calculated at this stage would be essential for the next stage, filtering out recommendations.

% The family of techniques most commonly used in recommender systems for information filtering is known as Collaborative Filtering. These techniques attempt to extract patterns and information using data sourced from several entities, users and agents such as those in my dataset. The scores obtained from my structuring of the data as explained in the above paragraph can be put to use here. I will start by experimenting with these techniques and then explore others, either variations of the standard CF methods or new novel ones. There is scope to put the focus of the information filtering on the users, the content (Cloudcasts), or find the point of balance that gives the best results. Deep analysis of either social behaviour of the user or the content will give rise to different sets of recommendations each of which may be suitable for different purposes.

\section{Success Criterion}

The main evaluation method I have opted for is that used to measure the performance of \textit{supervised learning} systems: the system written is fed a part of the dataset to learn from, known as the \textit{training set} which has complete information about known relationships between entities, and then a \textit{test set} is given with some relationships missing. The number of correct relationships that the system recommends as output can be used as a measure of how precise and successful the recommender is. Different metrics can be derived from such a test by identifying false negatives and false positives\footnote{http://www.cs.ualberta.ca/\textasciitilde eisner/measures.html -- this page describes the terms \textit{precision, accuracy, sensitivity} and \textit{specificity} with regard to such measurements.}. 

TODO: Incorporate cross-validation of partitioned dataset into explaination.

TODO: Temporal snapshots to measure quality of recommendations over time against user behaviour.

% Improvements in these and any other measures over the course of the project will mark its success and I expect to reach a reasonable, usable level of precision by the end.


\section{Resources Required}
See attached Project Resource Form. I have included a summary below:
\begin{itemize}
\item Mixcloud.com user and content data from their API -- a cautious estimated size of 2GB
\item PWF Filespace -- Up to 500Mbytes
\item SRCF Filespace  -- Up to 500Mbytes
\item My own Laptop PC (Intel Core 2 Duo T6400 2.0GHz, 4GB RAM, 500GB HHD)
\end{itemize}
For my extension ideas, I may also use some PC's over a network for high-performance processing of large datasets. For this I will use my Linksys 4-port router and out-of-circulation (but healthy) PC's from my college's Computer Support department.


\section{Timetable and Milestones}

From the official beginning of this project on Fri 23rd October until the submission deadline of Fri 20th May, there are 30 weeks which I have timetabled as follows:

TODO: Complete once the rest of the proposal is solid.

\textbf{22 Oct - Submission of this Proposal}

\textbf{23 Oct - 5 Nov}

% Obtain database, set it up for easy access and familiarise myself with it.
% Plan the structure of the recommender system, implementation language and tools based on literature and current solutions.

\textbf{6 Nov - 19 Nov}

% Practise using new tools or languages and implement simple algorithms.
% Have an understanding of the major methods and algorithms used currently and the mathematics behind them.
% Experiment with structuring and processing of data in n-partite graph form and note any observations about the usefulness of this technique.

\textbf{20 Dec - 3 Dec}

% Have a simple prototype coded that uses the data from the database and achieves simple but meaningful output, approaching a standard worthy of presenting at the Progress Report. The ``Slope One'' technique of Collaborative Filtering, one of the simplest, should by now be implemented, given that everything else has gone smoothly.

\textbf{4 Dec - 17 Dec}

% Extend the implementation of the simple CF techniques to more complex ones and try any new ones discovered in literature.
% Start planning out the dissertation. The beginning sections, Introduction and Preparation, should fleshed out by now.

\textbf{18 Dec - 7 Jan}

% Prepare presentation and code sufficiently for progress report.

\textbf{8 Jan - 21 Jan}

\textbf{22 Jan - 3 Feb}

\textbf{4 Feb - Submission of Progress Report}

\textbf{5 Feb - 18 Feb}

% Work on any feedback from progress report and begin experimenting hybrid techniques.
% Set out a clear plan for testing the performance of the recommender, including the type of measurements to be taken and comparisons to draw.

\textbf{19 Feb - 4 Mar}

% Analyse performance of code so far and move onto extensions if feasible.
% Final clean up of code and extensions.
% Dissertation document is regularly updated during this time and the Implementation section has taken form.

\textbf{5 Mar - 25 Mar}

% Dissertation is comfortably on target in terms of content and word limit.
% Identify weaknesses in code and write-up, then address them.

\textbf{26 Mar - 13 May}

Final proof reading of dissertation and aim to submit a week before deadline.

\textbf{20 May - Submission of Dissertation}
