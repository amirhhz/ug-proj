% to be included in proposal.tex

\author{Amir H. Hajizamani}
\title{CST Part II Individual Project Proposal - A Recommender System}


\newcommand{\al}{$<$}
\newcommand{\ar}{$>$}

\parindent 0pt
\parskip 6pt

\thispagestyle{empty}

\rightline{\large\emph{Amir H. Hajizamani}}
\medskip
\rightline{\large\emph{St. John's College}}
\medskip
\rightline{\large\emph{ahh29}}

\vfill

\centerline{\large Computer Science Tripos Part II Individual Project Proposal}
\vspace{0.4in}
\centerline{\Large\bf A Folksonomy-based Recommender System for Social Networks}
\vspace{0.3in}
\centerline{\large{22 October 2009}}

\vfill

{\bf Project Originators:} Nishanth Sastry and Mat Clayton

\vspace{0.1in}

{\bf Resources Required:} See attached Project Resource Form

\vspace{0.5in}

{\bf Project Supervisor:} Nishanth Sastry

\vspace{0.2in}

{\bf Signature:}

\vspace{0.5in}

{\bf Director of Studies:} Robert Mullins

\vspace{0.2in}

{\bf Signature:}

\vspace{0.5in}

{\bf Overseers:} J.A. Crowcroft and P. Lio

\vspace{0.2in}

{\bf Signatures:}

\vfill
\eject


\section{Introduction and Description of the Work}

In this project I will endeavour to write a recommender system to enhance the value of data available on a social network. The basis of this idea is that data about an individual person or some digital content (text, audio-visual, ratings, etc.) is only as valuable as its relation to other such entities. 

As a particular case, I will work on data provided by Mixcloud.com whom I worked for over the summer of 2009. They are a steadily-growing and successful content distribution website, aiming to be the ``YouTube of Radio'' and provide their users with community features such as Twitter-style following and updates, favouriting of ``Cloudcasts'' (uploaded content) and commenting. The user-uploaded ``Cloudcasts'' are also heavily and neatly tagged with genres and categories, and some carry text descriptions, too. I see all of this as rich but wasted data. My recommender system in this instance would provide the functionality of recommending Cloudcasts for a user based on the current Cloudcast's tags, the user's preferences and history, and furthermore it would recommend users whom they may be interested in following or listening to based on the same data.

The Web is rife with such systems already with companies such as \textit{Amazon, Netflix\footnote{In 2006 Netflix challenged programmers to improve their recommender system by 10\%.}} and \textit{Pandora} (radio service backed by \textit{The Music Genome Project}\footnote{http://www.pandora.com -- The MGP uses the``genealogy'' of music to link songs and artists.}) investing a lot of time and money in improving their algorithms and data quality to better recommend items of interest to their users. On the other hand, more directly community-driven websites such as \textit{del.icio.us} and \textit{flickr} have harnessed the collective intelligence of their users and their tagging of their content with keywords. Collaborative tagging leads to an organic categorisation and annotation of data which has been termed \textit{folksonomy}\footnote{http://www.vanderwal.net/folksonomy.html -- a portmanteau of \textit{folk} and \textit{taxonomy} }. In a sufficiently large and active community, the folksonomy that can be extracted becomes a good source of input for the learning algorithms in a recommender system.

\section{Extension Ideas}

Thinking ahead about possible extensions for this project, a limitation\footnote{http://www.readwriteweb.com/archives/recommendation\_systems\_where\_we\_need\_to\_go.php} of some current recommender systems is the latency of the recommendations after changes in user behaviour, item data and the relationships therein. Especially for large datasets a more ``real time'' performance would be desirable for a recommender system. A possible solution might be to periodically and incrementally update a stored set of recommendations after a sufficient number of changes occur. An algorithmic solution for this would be naturally followed by an attempt to increase the performance of the system with distributed processing across a cluster of computers.

In order to find out how my system performs with other datasets, I may be able to adapt it to perform similar functions on data from public API's such as \textit{last.fm}'s (tagged music data) and \textit{The Guardian}'s (tagged news stories and articles).

\section{Starting Point}

I am familiar with the database that I will be using following my internship at Mixcloud.com, and have an understanding of how the website's codebase is put together. This means I can keep the context of the system I will write in mind, in this particular case the Django web framework. What I have learned from some of the Computer Science Tripos courses that I have taken such as Artificial Intelligence I, Natural Language Processing and Databases will likely help me along the way.

Other than user experience of several system and websites already doing recommendations, the type of coding and algorithms involved in this project will be new to me.
  
\section{Substance and Structure of the Project}

The data for this project will be presented to me as MySQL databases. One of the first steps I will have to take is to identify any ways in which I have to manipulate these before I can use the data in my algorithms later on.

As I mentioned in the above section, I have no prior experience of the types of processing and algorithms used in building recommender systems and therefore I will have to spend some time examining the existing literature and open source code.

An initial idea as to how I might utilise my folksonomy data is by constructing bi- or tripartite graphs that link tags, content and users. The links between the items in these graphs can give us approximations of a user's preferences, or tell us which pieces of content may be related to each other, and a score can be computed about the strength of the connection between two items or users. It may be that several bipartite graphs (tags-content, content-users, users-users etc.) give better results than one very large tripartite graph and I would be interested to see how different approaches compare. The scores calculated at this stage would be essential for the next stage, filtering out recommendations.

The family of techniques most commonly used in recommender systems for information filtering is known as Collaborative Filtering. These techniques attempt to extract patterns and information using data sourced from several entities, users and agents such as those in my dataset. The scores obtained from my structuring of the data as explained in the above paragraph can be put to use here. I will start by experimenting with these techniques and then explore others, either variations of the standard CF methods or new novel ones. There is scope to put the focus of the information filtering on the users, the content (Cloudcasts), or find the point of balance that gives the best results. Deep analysis of either social behaviour of the user or the content will give rise to different sets of recommendations each of which may be suitable for different purposes.

\section{Success Criterion}

The main evaluation method I have opted for is that used to measure the performance of \textit{supervised learning} systems: the system written is fed a part of the dataset to learn from, known as the \textit{training set} which has complete information about known relationships between entities, and then a \textit{test set} is given with some relationships missing. The number of correct relationships that the system recommends as output can be used as a measure of how precise and successful the recommender is. Different metrics can be derived from such a test by identifying false negatives and false positives\footnote{http://www.cs.ualberta.ca/\textasciitilde eisner/measures.html -- this page describes the terms \textit{precision, accuracy, sensitivity} and \textit{specificity} with regard to such measurements.}. Improvements in these and any other measures over the course of the project will mark its success and I expect to reach a reasonable, usable level of precision by the end.


\section{Resources Required}
See attached Project Resource Form. I have included a summary below:
\begin{itemize}
\item Mixcloud.com user and content MySQL databases -- approximately 100Mbytes, or more with community growth
\item PWF Filespace -- Up to 500Mbytes
\item SRCF Filespace  -- Up to 500Mbytes
\item File space on GetDropBox.com (Subversion back-end) -- Up to 2250Mbytes
\item My own Laptop PC (Intel Core 2 Duo T6400 2.0GHz, 4GB RAM, 500GB HHD)
\end{itemize}
For my extension ideas, I may also use some PC's over a network for high-performance processing of large datasets. For this I will use my Linksys 4-port router and out-of-circulation Dell PC's from my college's Computer Support department.


\section{Timetable and Milestones}

From the official beginning of this project on Fri 23rd October until the submission deadline of Fri 14th May, there are 29 weeks which I have timetabled as follows:

\vfill
\eject

\textbf{23 Oct - 12 Nov}

Obtain database, set it up for easy access and familiarise myself with it.
Plan the structure of the recommender system, implementation language and tools based on literature and current solutions.

\textbf{13 Nov - 3 Dec}

Practise using new tools or languages and implement simple algorithms.
Have an understanding of the major methods and algorithms used currently and the mathematics behind them.
Experiment with structuring and processing of data in n-partite graph form and note any observations about the usefulness of this technique.

\textbf{4 Dec - 31 Jan}

Have a simple prototype coded that uses the data from the database and achieves simple but meaningful output, approaching a standard worthy of presenting at the Progress Report. The ``Slope One'' technique of Collaborative Filtering, one of the simplest, should by now be implemented, given that everything else has gone smoothly.

\textbf{1 Jan - 14 Jan}

Extend the implementation of the simple CF techniques to more complex ones and try any new ones discovered in literature.
Start planning out the dissertation. The beginning sections, Introduction and Preparation, should fleshed out by now.

\textbf{8 Jan - 28 Jan}

Prepare presentation and code sufficiently for progress report.

\textbf{29 Jan - Submission of Progress Report}

\textbf{30 Jan - 18 Feb}

Work on any feedback from progress report and begin experimenting hybrid techniques.
Set out a clear plan for testing the performance of the recommender, including the type of measurements to be taken and comparisons to draw.

\textbf{19 Feb - 11 Mar}

Analyse performance of code so far and move onto extensions if feasible.
Final clean up of code and extensions.
Dissertation document is regularly updated during this time and the Implementation section has taken form.

\textbf{12 Mar - 1 Apr}

Dissertation is comfortably on target in terms of content and word limit.
Identify weaknesses in code and write-up, then address them.

\textbf{2 Apr - 7 May}

Final proof reading of dissertation and aim to submit a week before deadline.

\textbf{14 May - Submission of Dissertation}
